uppercase,exclamation,has_exclamation,question,words_per_sentence,adj,adv,noun,spell_errors,lexical_size,polarity,number_sentences,len_text,word2vec,Text,label
0.02,0.0,0.0,0.0,21.76,0.08,0.05,0.22,0.03,260,0.2,21.0,2752.0,2.158539056777954,"Catherine R. Squires is a professor of communication studies at the University of Minnesota, Twin Cities. She is also the director of the Race, Indigeneity, Gender and Sexuality Studies Initiative.    Facebook is a for-profit company that makes money packaging its users' information to sell to advertisers and other entities. The company's goal is not to produce a ""balanced"" information diet for its users. People who are shocked that Facebook might be skewing their newsfeed probably shouldn't have trusted them with their news diet in the first place, given its history. Remember those confusing and ever-changing privacy settings, and that experiment to see whether users' moods could be manipulated by changing the newsfeed? This is not the company I'd trust to tell me what's important in the world.    But the uproar over the role of human editors at Facebook — or at least, in the ""Trending Topics"" section — does revive an important question: In an information age when people can customize their news diet, how should Facebook editors decide what issues, opinions or events deserve prominence?    Given their newfound reliance on social media companies like Facebook, traditional media editors have been grappling with the same question. Any news publication with a website makes ad revenue off of popular articles, but that can be a dangerous incentive. Though important news can also be popular, all of the major publications are guilty of publishing dumbed down ""clickbait"" to attract wider audiences.    So then, perhaps the question is whether Facebook, or even the news media, is narrowing the field of news so that we, as citizens, are unable to engage in effective political and social discussions.    A Facebook newsfeed that was completely dictated by algorithms without human interference wouldn't be any better. Algorithms reflect the imperfect biases of the humans that build them. Algorithms rely on data sets, which are structured by the decisions of data gatherers guided by particular goals. For example, as most of the people who work in computing are male, it's not surprising that scholars found a gender bias in the Google's Image search: In searches for C.E.O., 11 percent of the people depicted were women, compared with 27 percent of U.S. C.E.O.s who are actually women.    Data and news can be skewed on many levels on the Internet, but competent editors could explain how they and their algorithms work. Then, at least, the public would know how and why news sites elevate certain stories.    But bias in the media is not new. These basic questions have to be worked out by each generation, confronted by each new development in media technology. They are ethical and practical questions that require a human touch.",1.0
